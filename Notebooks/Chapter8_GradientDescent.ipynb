{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJIGQoB0bu9XV8IswzKz3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravichas/bifx-546/blob/main/Notebooks/Chapter8_GradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent (GD)\n",
        "\n",
        "GD is a method to finding the minimum of a function (lowest point). This is often explained as similar to walking downhill on a cloudy mountain.\n",
        "\n",
        "In simple words, from your current position, you look at the slope (the gradient) to see which direction goes down the fastest. Then you take a small step in that direction. You repeat this step until your steps dont get you lower.\n",
        "\n",
        "In ML, this is how we adjust model parameters (like weights in NN) to reduce error on the training data\n",
        "\n"
      ],
      "metadata": {
        "id": "HRCJGINw26UA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Do We Need Gradient Descent?\n",
        "\n",
        "From the book,\n",
        "\n",
        "`We want to minimize (input a value(s) that make the function output as small as possible) a function, but we dont have a closed-form solution`\n",
        "\n",
        "## Example of a closed-form solution\n",
        "\n",
        "Consider:\n",
        "Minimization of\n",
        "$$f(x) = x^2$$\n",
        "\n",
        "Take derivative\n",
        "$$ df/dx = 2x $$\n",
        "\n",
        "Set derivative to zero:\n",
        "\n",
        "2x = 0 -> x = 0\n",
        "\n",
        "## Example of a function without closed form solution\n",
        "\n",
        "$$ f(x) = x^4 + x^2 + sin(x) $$\n",
        "\n",
        "You can take the derivative but solving (df/dx=0) will not result in a clean form. You can find a numerical solution.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def square(x):\n",
        "  return x * x\n",
        "```\n",
        "inputs: x\n",
        "output: x$^2$\n",
        "question: which x gives the smallest value of $x^2$? (x = 0)\n",
        "Function: square(x)\n",
        "Minimum -> lowest point of the curve\n",
        "\n",
        "In datascience and ML:\n",
        "* The function is usually error/loss function\n",
        "* inputs: model parameters\n",
        "* output: measures how bad the model is\n",
        "\n",
        "So, minimizing a function means:\n",
        "`Choose model parameters that make the model's error as small as possible`\n",
        "\n",
        "\n",
        "\n",
        "Examples:\n",
        "* Minimize sum of squared errors\n",
        "* Fit a line by choosing parameters\n",
        "* Tune model parameters\n",
        "\n",
        "Questions to ask:\n",
        "* What if the funciton is complicated?\n",
        "* What if there are many parameters\n",
        "* What if derivatives exist but solutions dont?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VSuBUeKlmcM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why this matters\n",
        "\n",
        "In real ML problems:\n",
        "* Loss functions are complicated\n",
        "* High Dimensional\n",
        "* often sums of millions of terms\n",
        "\n",
        "Even if derivatives exist:\n",
        "* Solving them exactly is impossible\n",
        "\n"
      ],
      "metadata": {
        "id": "T7jxJT0D31ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Details of SD\n",
        "\n",
        "* Data (x1, y1), (x2, y2), ..., (xn, yn)\n",
        "* Assume a linear model, y = mx + b\n",
        "* We need to decide what the loss function is, Mean squared Error\n",
        "* Batch size (subset; mini-batch)\n",
        "\n",
        "## complete setup\n",
        "\n",
        "* initialize: m = 0, b = 0\n",
        "* LR = 0.001\n",
        "* Batch size = 100\n",
        "* Total data = 1000 points\n",
        "* Total # of batches per epoch: 1000/100 = 10\n",
        "\n",
        "Epoch: 1\n",
        "\n",
        "Batch 1 (samples 1-100)\n",
        "* Step 1: get batch data\n",
        "  ```\n",
        "  x_batch = [x1, x2, ..., x100]\n",
        "  y_batch = [y1, y2, ..., y100]\n",
        "  ```\n",
        "\n",
        "* Step2:  L = MSQ error\n",
        "Loss function: $$L = (1/100) \\sum_{i=1}^{100}(mx_{i} + b - y_{i})^2 $$\n",
        "\n",
        "  Parameters to optimize: m and b\n",
        "\n",
        "  Gradient formulas:\n",
        "\n",
        "  ($\\partial L/\\partial m)$\n",
        "\n",
        "  ($\\partial L/\\partial b)$\n",
        "\n",
        "   dL_dm = (2/n) * sum(m * x[i] + b - y[i]) * x[i])\n",
        "\n",
        "   dL_db = (2/n) * sum(m * x[i] + b - y[i]))\n",
        "\n",
        "   $\\text{dL_dm} = (2/100) \\sum_{i=1}^{100} (m x_{i} + b - y_i) * x_{i}$\n",
        "\n",
        "  $\\text{dL_db} = (2/100) \\sum_{i=1}^{100} (m x_{i} + b - y_i)$\n",
        "\n",
        "* Step3: update m and b\n",
        "\n",
        "  m = m_old - lr * dl_dm\n",
        "\n",
        "  suppose if we get lr = 0.001; dL/dm = -13 and dL/db = -7\n",
        "\n",
        "  m = 0 - 0.001 * (-13) = 0 + 0.013 (m_old (beginning m) = 0)\n",
        "  \n",
        "  b = b_old - lr * dl_db\n",
        "  (one could calculate b)\n",
        "\n",
        "  Justification of formula for update\n",
        "\n",
        "  if $\\partial L/\\partial m = +ve (say +5); lr = 0.001\n",
        "    * Loss increases if we increase m\n",
        "    * update: m_new = m_old - 0.001 * (+5) = m_old - 0.005\n",
        "    * Right thing is to decrease m\n",
        "  \n",
        "  if $\\partial L/\\partial m = -ve (say -5)\n",
        "  * Loss increases if we increase m\n",
        "  * update: m_new = m_old - 0.001 * (-5) = m_old + 0.005\n",
        "  * Right thing is to increase m\n",
        "\n",
        "* step 4: Batch 2 (samples 101-200)\n",
        "\n",
        "  ```\n",
        "  x_batch = [x101, x102, ..., x200]\n",
        "  y_batch = [y101, y102, ..., y200]\n",
        "  ```\n",
        "\n",
        "  Compute loss on this second batch\n",
        "\n",
        "  $L = (1/100) \\sum_{i=101}^{200}(m x_{i} + b - y_{i})^2 $\n",
        "\n",
        "  Note: Use updated m and b calculated in the previous step\n",
        "\n",
        "* Step 5: Compute gradients\n",
        "\n",
        "  $\\text{dL_dm} = (2/100) \\sum_{i=101}^{200} (m x_{i} + b - y_i) * x_{i}$\n",
        "\n",
        "  $\\text{dL_db} = (2/100) \\sum_{i=101}^{200} (m x_{i} + b - y_i)$\n",
        "\n",
        "  use the new dl_dm and dl_db to update m and b. For m_old and b_old use the previously updated m and b\n",
        "\n",
        "* Step 6: Batches 3 - 10\n",
        "\n",
        "   Repate the same process for rest of the batches (201-300 .... 901-1000)\n",
        "\n",
        "* This will conclude one epoch (Epoch 1)\n",
        "  * 1000 data points in 10 batches of 100\n",
        "  * parameters updated 10 times (once per batch\n",
        "  * current values m and b (after 10 updates are the final m and b)\n",
        "\n",
        "Before Epoch 2\n",
        "\n",
        "  * shuffle the data (important)\n",
        "  * Split the data into 10 mini batches (new)\n",
        "  * Repeat the process described above\n",
        "\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "TqMzh3L0PGan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Many different method\n",
        "\n",
        "\n",
        "|Method|Batch-size|Gradient Computed from| Updates/Epoch|\n",
        "|----|----|-----|---|\n",
        "|Batch GD | 1000 (all) |  All 1K samples| 1 |\n",
        "|SGD | 1 | Single Sample| 1000 |\n",
        "|Mini-Batch| 100 | 100 samples| 10|\n",
        "\n",
        "\n",
        "# Advantages of Batches\n",
        "\n",
        "1. Faster than Batch GD\n",
        "2. Less noisy (more stable)\n",
        "3. Memory efficient (dont need to load all data at once)\n",
        "4. GPU friendly\n",
        "\n",
        "```\n",
        "for epoch in 1 to num_epochs:\n",
        "   shuffle(data)\n",
        "\n",
        "   for each batch in data:\n",
        "      # forward pass\n",
        "      predictions = m * x_batch + b\n",
        "      loss = mean((predictions - y_batch)^2)\n",
        "\n",
        "      # compuate GR (only for this batch)\n",
        "      dL_dm = (2/batch_size) * sum(predictions - y_batch) * x_batch\n",
        "      dL_db = 2/batch_size) * sum(predictions - y_batch)\n",
        "\n",
        "      # update\n",
        "      m = m - lr * dL_dm\n",
        "      b = b - lr * dL_db\n",
        "```\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "d6-iEjKWa1OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Course setup: safe clone + cd + import path ====\n",
        "import os\n",
        "import sys\n",
        "\n",
        "REPO_URL = \"https://github.com/joelgrus/data-science-from-scratch.git\"\n",
        "REPO_DIR = \"data-science-from-scratch\"\n",
        "\n",
        "# 1. If we're *anywhere inside* the repo, move to the parent directory first\n",
        "cwd = os.getcwd()\n",
        "if REPO_DIR in cwd.split(os.sep):\n",
        "    parts = cwd.split(os.sep)\n",
        "    # Walk up until we are at .../data-science-from-scratch\n",
        "    while parts and parts[-1] != REPO_DIR:\n",
        "        parts.pop()\n",
        "    # Now go to the directory *above* the repo\n",
        "    parent_dir = os.sep.join(parts[:-1]) or \"/\"\n",
        "    os.chdir(parent_dir)\n",
        "    print(f\"Moved to parent directory: {os.getcwd()}\")\n",
        "\n",
        "# 2. Clone only if needed\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(\"Cloning repo...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(f\"{REPO_DIR} already exists — skipping clone.\")\n",
        "\n",
        "# 3. cd into the repo (this is where you'll live most of the time)\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "# 4. Ensure the repo is importable (for scratch.linear_algebra, etc.)\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.append(os.getcwd())\n",
        "    print(\"Added repo to sys.path\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F1HMBFEEeyc",
        "outputId": "f7334842-fc28-4166-95f8-5d3956b07b5a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved to parent directory: /content\n",
            "data-science-from-scratch already exists — skipping clone.\n",
            "/content/data-science-from-scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of the following code\n",
        "\n",
        "* `Callable[ [ArgType1, ArgType2, ...], ReturnType1 `\n",
        "* Example,  `f: Callable[ [float], float]` means that f is a function that takes one arg (either x or  (x+h) and returns one result, float"
      ],
      "metadata": {
        "id": "wMGAJANHFMnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "def difference_quotient(f: Callable[[float], float],\n",
        "                        x: float, h: float) -> float:\n",
        "  return (f(x + h) - f(x)) / h"
      ],
      "metadata": {
        "id": "6J9_4VkmEvQF"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from scratch.linear_algebra import Vector, dot\n",
        "\n",
        "from scratch.linear_algebra import distance, add, scalar_multiply\n",
        "\n",
        "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
        "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
        "    assert len(v) == len(gradient)\n",
        "    step = scalar_multiply(step_size, gradient)\n",
        "    return add(v, step)\n",
        "\n",
        "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
        "    return [2 * v_i for v_i in v]"
      ],
      "metadata": {
        "id": "7GbMg-Ki11ky"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x ranges from -50 to 49, y is always 20 * x + 5\n",
        "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]"
      ],
      "metadata": {
        "id": "U7CJRIYt2DUO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBchuf7M2FGy",
        "outputId": "4b63214d-dc2a-4f9c-bc5e-dd4270d38dd1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-50, -995),\n",
              " (-49, -975),\n",
              " (-48, -955),\n",
              " (-47, -935),\n",
              " (-46, -915),\n",
              " (-45, -895),\n",
              " (-44, -875),\n",
              " (-43, -855),\n",
              " (-42, -835),\n",
              " (-41, -815)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic information\n",
        "\n",
        "* x: input (feature)\n",
        "* y: observed output (label)\n",
        "* m, b: model parameters (slope and intercept)\n",
        "\n",
        "The model is: $\\hat{y} = m x + b$\n",
        "\n",
        "# 1) Model and loss\n",
        "\n",
        "You have a linear model:\n",
        "* parameters: $\\theta$ = (m, b) where m = slope, b = intercept\n",
        "* prediction:\n",
        "$\\hat y = m x + b$\n",
        "* error:\n",
        "$$e = \\hat{y} - y = (mx + b - y)$$\n",
        "* Our goal is to find values of m and b that make the model’s predictions $\\hat {y}$ as close as possible to the observed y.\n",
        "\n",
        "* We measure “closeness” using a loss function, here:\n",
        "$L(m,b) = (mx + b - y)^2$\n",
        "* squared error loss for one data point:\n",
        "$L(m,b) = e^2 = (mx + b - y)^2$\n",
        "* Why we take derivatives wrt m and b\n",
        "  * x and y are data (given, fixed)\n",
        "\t* m and b are variables we can change\n",
        "\n",
        "  So we ask:\n",
        "  *\t“If I slightly change m, does the loss go up or down?”\n",
        "\t*\t“If I slightly change b, does the loss go up or down?”\n",
        "  \n",
        "  That’s why the gradient is taken with respect to m and b, not with respect to x or y.\n",
        "  \n",
        "We want $\\nabla_\\theta L = \\left[\\frac{\\partial L}{\\partial m},\\frac{\\partial L}{\\partial b}\\right]$."
      ],
      "metadata": {
        "id": "V9ur7Dk5DaWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Partial wrt m (slope)\n",
        "\n",
        "$\\frac{\\partial L}{\\partial m}$\n",
        "\n",
        "Answers:\n",
        "\n",
        "“If I change the slope slightly, keeping the intercept fixed, how does the loss change?”\n",
        "\n",
        "That’s why it depends on:\n",
        "* the error\n",
        "* the input value x (because slope affects prediction via x)\n",
        "\n",
        "## Derivation\n",
        "\n",
        "Use chain rule. Let e = mx + b - y, so $L = e^2$.\n",
        "\n",
        "$\\frac{\\partial L}{\\partial m} = \\frac{\\partial (e^2)}{\\partial e}\\cdot \\frac{\\partial e}{\\partial m}$\n",
        "*\t$\\frac{\\partial (e^2)}{\\partial e} = 2e$\n",
        "* $\\frac{\\partial e}{\\partial m} = x$ (because e = mx + b - y)\n",
        "\n",
        "So:\n",
        "$\\frac{\\partial L}{\\partial m} = 2e \\cdot x$\n",
        "\n",
        "That matches 2 * error * x.\n"
      ],
      "metadata": {
        "id": "MBaOHAHEGUW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial wrt b (intercept)\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "##Answers:\n",
        "\n",
        "“If I shift the line up or down slightly, keeping the slope fixed, how does the loss change?”\n",
        "\n",
        "That’s why:\n",
        "* it depends on the error\n",
        "* but not on x\n",
        "\n",
        "## Derivation\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b} = \\frac{\\partial (e^2)}{\\partial e}\\cdot \\frac{\\partial e}{\\partial b}$\n",
        "* $\\frac{\\partial (e^2)}{\\partial e} = 2e$\n",
        "* $\\frac{\\partial e}{\\partial b} = 1$\n",
        "\n",
        "So:\n",
        "$\\frac{\\partial L}{\\partial b} = 2e$\n",
        "\n",
        "That matches 2 * error.\n"
      ],
      "metadata": {
        "id": "bs1mrCbmGfHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
        "    slope, intercept = theta\n",
        "    predicted = slope * x + intercept    # The prediction of the model.\n",
        "    error = (predicted - y)              # error is (predicted - actual)\n",
        "    squared_error = error ** 2           # We'll minimize squared error\n",
        "    grad = [2 * error * x, 2 * error]    # using its gradient.\n",
        "    return grad"
      ],
      "metadata": {
        "id": "MtoyD8vK2RMw"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meaning of  `TypeVar`\n",
        "\n",
        "`dataset: List[T]`\n",
        "\n",
        "Means:\n",
        "* dataset is a list of **some type**\n",
        "* All elements have the *same type*\n",
        "* That type is called T\n",
        "* TypeVar says, \"whatever type goes in, the same type comes out - whatever the type it is\""
      ],
      "metadata": {
        "id": "kOgsW5QPmPao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "from typing import TypeVar, List, Iterator\n",
        "\n",
        "# Step 1: Define a TypeVar\n",
        "T = TypeVar('T')\n",
        "\n",
        "# Step 2: Define minibatches using T\n",
        "def minibatches(dataset: List[T], batch_size: int) -> Iterator[List[T]]:\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        yield dataset[i:i + batch_size]\n",
        "        # give me one result, and remember where you were\n",
        "        # builds and return the list upon return\n",
        "\n",
        "# Step 3: Call minibatches with a concrete dataset\n",
        "numbers: List[int] = [10, 20, 30, 40]\n",
        "\n",
        "# Step 4: Use the result\n",
        "for batch in minibatches(numbers, batch_size=2):\n",
        "    print(batch, type(batch[0]))\n",
        "\n",
        "# return [ [1,2], [3, 4]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrWaDqTpn0Jx",
        "outputId": "c6b9c6ec-0bd6-49b0-a0eb-9c58e14c3d2e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 20] <class 'int'>\n",
            "[30, 40] <class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypeVar, List, Iterator\n",
        "\n",
        "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
        "\n",
        "def minibatches(dataset: List[T],\n",
        "                batch_size: int,\n",
        "                shuffle: bool = True) -> Iterator[List[T]]:\n",
        "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
        "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
        "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
        "\n",
        "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
        "\n",
        "    for start in batch_starts:\n",
        "        end = start + batch_size\n",
        "        yield dataset[start:end]\n"
      ],
      "metadata": {
        "id": "yhIlK7JK4Q7d"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimate Derivatives and Comparing it with True Values"
      ],
      "metadata": {
        "id": "983utVc-07TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "\n",
        "# note already defined in scratch/linear_algebra.py\n",
        "# from typing import List\n",
        "# Vector = List[float]\n",
        "\n",
        "print(type(Vector))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr20iWf-8428",
        "outputId": "5039b8ac-209a-4a9d-f7af-97578e999801"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'typing._GenericAlias'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1.0, 2.0, 3.0]\n",
        "v: Vector = my_list\n",
        "\n",
        "print(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3VnTJz--aLT",
        "outputId": "1eb3a1d0-eddc-453f-bb07-8c85111f0c6f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 2.0, 3.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example code chunks"
      ],
      "metadata": {
        "id": "MWuhOVrq5fTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "count: int = 11\n",
        "price: float = 9.99\n",
        "name: str = \"Hello\"\n",
        "is_active: bool = True\n",
        "\n",
        "# collections\n",
        "numbers: List[int] = [1, 2, 3]\n",
        "coordinates: Tuple[float, float] = [1.5, 2.5]\n",
        "scores: Dict[str, float] = {\"Alice\": 95.5, \"Bob\": 87.3}\n",
        "\n",
        "# custom type\n",
        "#Vector = List[float] # already defined\n",
        "position: Vector = [1.0, 2.0, 3.0]\n",
        "\n",
        "# Optional can be None)\n",
        "middle_name: Optional[str] = None"
      ],
      "metadata": {
        "id": "JswnVFrgAab3"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def difference_quotient(f: Callable[[float], float],\n",
        "                        x: float,\n",
        "                        h: float) -> float:\n",
        "    return (f(x + h) - f(x)) / h\n",
        "\n",
        "def square(x: float) -> float:\n",
        "    return x * x\n",
        "\n",
        "def derivative(x: float) -> float:\n",
        "    return 2 * x\n",
        "\n",
        "def estimate_gradient(f: Callable[[Vector], float],\n",
        "                      v: Vector,\n",
        "                      h: float = 0.0001):\n",
        "    return [partial_difference_quotient(f, v, i, h)\n",
        "            for i in range(len(v))]\n",
        "\n",
        "def partial_difference_quotient(f: Callable[[Vector], float],\n",
        "                                v: Vector,\n",
        "                                i: int,\n",
        "                                h: float) -> float:\n",
        "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
        "    w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\n",
        "          for j, v_j in enumerate(v)]\n",
        "    print('w', w)\n",
        "    return (f(w) - f(v)) / h"
      ],
      "metadata": {
        "id": "z9s2eHeL4Z7L"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xs = range(-10, 11)\n",
        "actuals = [derivative(x) for x in xs]\n",
        "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
        "\n",
        "# plot to show they're basically the same\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title(\"Actual Derivatives vs. Estimates\")\n",
        "plt.plot(xs, actuals, 'rx', label='Actual')       # red  x\n",
        "plt.plot(xs, estimates, 'b+', label='Estimate')   # blue +\n",
        "plt.legend(loc=9)\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "gZtZdvXC4Zo_",
        "outputId": "d8eb08e5-a3ae-4f76-a87f-cef745551706"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7d0bfc956ea0>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGzCAYAAAABsTylAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUZFJREFUeJzt3Xt4E2XaP/BvCG1aoAdKW5JqOZWTKFRSpRYXgbVysLiiiMDrvnJqaAR0RUXB90cb2sUqILDrgoGAwK6snARFUFxkAQ+UCkEUEFhgy7HTIiBtAWlp+vz+GDIQeqCFJpm038915QrzzGRyTyc0d2ee5340QggBIiIiIhVq4O0AiIiIiCrDRIWIiIhUi4kKERERqRYTFSIiIlItJipERESkWkxUiIiISLWYqBAREZFqMVEhIiIi1WKiQkRERKrFRIWoFmk0GlgsFm+HUaFjx45Bo9FgyZIlXnl/Nf9s6iqLxQKNRuPtMIjuCBMVUq158+ZBo9EgPj7+tveRm5sLi8WCPXv21F5gd8iZMDgffn5+CA8PR/fu3fHmm2/ixIkT3g7xtn3++edMRqrh5s/AzY+333672vu6fPkyLBYLtm7d6r6AbwM/C1RbNJzrh9Tq4YcfRm5uLo4dO4bDhw+jbdu2Nd7Hrl278OCDD2Lx4sUYMWJE7Qd5E41Gg7S0tCp/QR87dgytW7fGsGHD8Pjjj6OsrAy//vordu7ciTVr1kCj0WDRokUYOnRorcYmhEBxcTH8/Pyg1Wprdd9O48ePx9y5c1HRr5UrV66gYcOGaNiwoVve25fc/Bm4WdeuXXHvvfdWa19nz55FREREhZ+70tJSlJaWIiAgoDbCrpGqPgtENcHfGKRKOTk52L59O9asWYOUlBQsW7YMaWlp3g6rVhmNRvzxj390aTt+/Dj69OmD4cOH45577kFsbOwdv09paSnKysrg7+/vlS8sJ2++t1pV9BmoTUwMqS7grR9SpWXLlqFp06ZISkrCM888g2XLllW43YULFzBhwgS0atUKOp0Od999N55//nmcPXsWW7duxYMPPggAGDlypHJZ3dlHo1WrVhVeZenVqxd69eqlLJeUlCA1NRVxcXEICQlB48aN0aNHD2zZsqW2DxstW7bEkiVLUFJSgunTp5c71pdffhnR0dHQ6XRo27Yt3nnnHZSVlSnbOG8pzJw5E3PmzEFMTAx0Oh1+/vnncn1UZs6cCY1Gg+PHj5eLY/LkyfD398evv/4KAPjmm28wePBgtGjRAjqdDtHR0ZgwYQJ+++035TUjRozA3LlzAcDlNobTjX1UVq9eDY1Gg23btpV77/nz50Oj0WDfvn1K28GDB/HMM88gLCwMAQEBeOCBB7Bu3TqX1129ehVTp05Fu3btEBAQgGbNmuF3v/sdNm3aVOnPe9euXdBoNFi6dGm5dV9++SU0Gg3Wr18PACgqKsLLL7+sfNYiIyPx2GOPYffu3ZXuvzbs2rULffv2RXh4OAIDA9G6dWuMGjUKgHy+IyIiAABTp05VfubOn3NFfVQ0Gg3Gjx+PVatWoVOnTggMDERCQgL27t0LQP75t23bFgEBAejVqxeOHTvm8vra+CyUlZVhzpw5uPfeexEQEIDmzZsjJSVF+bxV59ip/mCqTaq0bNkyPP300/D398ewYcPw/vvvY+fOnUriAQAXL15Ejx49cODAAYwaNQpGoxFnz57FunXrcOrUKdxzzz1IT09HamoqxowZgx49egAAunfvXqNYCgsLsXDhQgwbNgwmkwlFRUVYtGgR+vbti++//x73339/bR46EhISEBMT4/IFe/nyZfTs2ROnT59GSkoKWrRoge3bt2Py5MmQJAlz5sxx2cfixYtx5coVjBkzBjqdDmFhYS4JDQA8++yzeP3117Fy5UpMnDjRZd3KlSvRp08fNG3aFACwatUqXL58GS+88AKaNWuG77//Hu+99x5OnTqFVatWAQBSUlKQm5uLTZs24R//+EeVx5iUlIQmTZpg5cqV6Nmzp8u6FStW4N5778V9990HANi/fz8efvhh3HXXXZg0aRIaN26MlStXYuDAgfj444/x1FNPAZC/lDMzM5GcnIxu3bqhsLAQu3btwu7du/HYY49VGMcDDzyANm3aYOXKlRg+fHi5OJo2bYq+ffsCAMxmM1avXo3x48ejU6dOOHfuHL799lscOHAARqOxyuOtzOXLl3H27Nly7aGhoWjYsCHOnDmDPn36ICIiApMmTUJoaCiOHTuGNWvWAAAiIiLw/vvv44UXXsBTTz2Fp59+GgDQpUuXKt/3m2++wbp16zBu3DgAQGZmJgYMGIDXX38d8+bNw9ixY/Hrr79i+vTpGDVqFP79738rr62Nz0JKSgqWLFmCkSNH4qWXXkJOTg7+9re/4YcffsB3330HPz+/Wx471SOCSGV27dolAIhNmzYJIYQoKysTd999t/jTn/7ksl1qaqoAINasWVNuH2VlZUIIIXbu3CkAiMWLF5fbpmXLlmL48OHl2nv27Cl69uypLJeWlori4mKXbX799VfRvHlzMWrUKJd2ACItLa3K48vJyREAxIwZMyrd5sknnxQAREFBgRBCiIyMDNG4cWPxn//8x2W7SZMmCa1WK06cOOGy7+DgYHHmzJkK3/fGn0VCQoKIi4tz2e77778XAMTf//53pe3y5cvlYszMzBQajUYcP35caRs3bpyo7NfKzT+bYcOGicjISFFaWqq0SZIkGjRoINLT05W2Rx99VHTu3FlcuXJFaSsrKxPdu3cX7dq1U9piY2NFUlJShe9dlcmTJws/Pz9x/vx5pa24uFiEhoa6nN+QkBAxbty4Gu+/Is5zUdkjKytLCCHE2rVrBQCxc+fOSvf1yy+/VPq5S0tLK3c+AAidTidycnKUtvnz5wsAQq/Xi8LCQqV98uTJAoDLtnf6Wfjmm28EALFs2TKX9o0bN7q0V+fYqX7grR9SnWXLlqF58+bo3bs3APnS8ZAhQ7B8+XI4HA5lu48//hixsbHKX9Q3qs0hmVqtFv7+/gDkS9bnz59HaWkpHnjgAbdd9m/SpAkA+XYDIP8V26NHDzRt2hRnz55VHomJiXA4HPj6669dXj9o0CDllkBVhgwZArvdjqNHjyptK1asgE6nw5NPPqm0BQYGKv++dOkSzp49i+7du0MIgR9++OG2jnHIkCE4c+aMy2iV1atXo6ysDEOGDAEAnD9/Hv/+97/x7LPPoqioSDnuc+fOoW/fvjh8+DBOnz4NQL4KsX//fhw+fLjGcVy9etXlL/V//etfuHDhghKHc//Z2dnIzc29reOtyJgxY7Bp06Zyj06dOinvCQDr16/H1atXa+19H330UbRq1UpZdo6sGzRoEIKCgsq1//e//1Xa7vSzsGrVKoSEhOCxxx5z+SzHxcWhSZMmyi1Vdx07+R4mKqQqDocDy5cvR+/evZGTk4MjR47gyJEjiI+PR35+PjZv3qxse/ToUeX2gLstXboUXbp0Ufo+REREYMOGDSgoKHDL+128eBEAlC+Nw4cPY+PGjYiIiHB5JCYmAgDOnDnj8vrWrVtX630GDx6MBg0aYMWKFQDkkUGrVq1C//79ERwcrGx34sQJjBgxAmFhYWjSpAkiIiKUWza3+zPo168fQkJClPcG5CTp/vvvR/v27QEAR44cgRACU6ZMKXfszs7VzmNPT0/HhQsX0L59e3Tu3BkTJ07ETz/9dMs4YmNj0bFjx3JxhIeH4/e//73SNn36dOzbtw/R0dHo1q0bLBaLyxf47WjXrh0SExPLPZw/+549e2LQoEGYOnUqwsPD8eSTT2Lx4sUoLi6+o/dt0aKFy3JISAgAIDo6usL2G/uO3Oln4fDhwygoKEBkZGS5c3rx4kXlfLrr2Mn3sI8Kqcq///1vSJKE5cuXY/ny5eXWL1u2DH369KmV96rsqovD4XAZvvvhhx9ixIgRGDhwICZOnIjIyEhotVpkZma6XImoTfv27UNkZKTyhVVWVobHHnsMr7/+eoXbO7/YnW78q7cqUVFR6NGjB1auXIk333wTO3bswIkTJ/DOO+8o2zgcDjz22GM4f/483njjDXTs2BGNGzfG6dOnMWLEiHJ9X6pLp9Nh4MCBWLt2LebNm4f8/Hx89913eOutt5RtnPt+7bXXlL4iN3MOW3/kkUdw9OhRfPrpp/jXv/6FhQsXYvbs2bBarUhOTq4yliFDhmDatGk4e/YsgoKCsG7dOgwbNsxlxMyzzz6LHj16YO3atfjXv/6FGTNm4J133sGaNWvQv3//2/oZ3IpGo8Hq1auxY8cOfPbZZ/jyyy8xatQovPvuu9ixY4dy5a2mKhueXlm7uDbEuDY+C2VlZYiMjKy0g7zzSqC7jp18DxMVUpVly5YhMjJSGTFwozVr1mDt2rWwWq0IDAxETEyMy8iQilR1C6hp06a4cOFCufbjx4+jTZs2yvLq1avRpk0bpcaJk7uGS2dlZeHo0aMuw1ZjYmJw8eJF5QpKbRoyZAjGjh2LQ4cOYcWKFWjUqBGeeOIJZf3evXvxn//8B0uXLsXzzz+vtFc0mqamt9yGDBmCpUuXYvPmzThw4ACEEC63W5znwc/Pr1rHHhYWhpEjR2LkyJG4ePEiHnnkEVgslmolKlOnTsXHH3+M5s2bo7CwsMI6NgaDAWPHjsXYsWNx5swZGI1GTJs2zW2JitNDDz2Ehx56CNOmTcM///lPPPfcc1i+fDmSk5M9Wnm2Nj4LMTEx+Oqrr/Dwww9XK6Gu6tipfuCtH1KN3377DWvWrMGAAQPwzDPPlHuMHz8eRUVFyrDUQYMG4ccff8TatWvL7cv5F2Djxo0BoMKEJCYmBjt27EBJSYnStn79epw8edJlO+dfmeKGwlXZ2dnIysq6swOuwPHjxzFixAj4+/u7jMR59tlnkZWVhS+//LLcay5cuIDS0tLbfs9BgwZBq9Xio48+wqpVqzBgwADl5wZUfPxCCPzlL38pt6+qft4VSUxMRFhYGFasWIEVK1agW7duLretIiMj0atXL8yfPx+SJJV7/S+//KL8+9y5cy7rmjRpgrZt21brVsE999yDzp07K3EYDAY88sgjynqHw1HutkZkZCSioqJc9n/27FkcPHgQly9fvvXBV8Ovv/5armCac5SZ830bNWoEoPo/8ztRG5+FZ599Fg6HAxkZGeVeU1paqmxfnWOn+oFXVEg11q1bh6KiIvzhD3+ocP1DDz2EiIgILFu2DEOGDMHEiROxevVqDB48GKNGjUJcXBzOnz+PdevWwWq1IjY2FjExMQgNDYXVakVQUBAaN26M+Ph4tG7dGsnJyVi9ejX69euHZ599FkePHsWHH36ImJgYl/cdMGAA1qxZg6eeegpJSUnIycmB1WpFp06dlL4kt2P37t348MMPUVZWhgsXLmDnzp34+OOPodFo8I9//MNliOnEiROxbt06DBgwACNGjEBcXBwuXbqEvXv3YvXq1Th27BjCw8NvK47IyEj07t0bs2bNQlFRkcsVDQDo2LEjYmJi8Nprr+H06dMIDg7Gxx9/XK7mBQDExcUBAF566SX07dsXWq22ygq7fn5+ePrpp7F8+XJcunQJM2fOLLfN3Llz8bvf/Q6dO3eGyWRCmzZtkJ+fj6ysLJw6dQo//vgjAKBTp07o1asX4uLiEBYWhl27dinDiatjyJAhSE1NRUBAAEaPHo0GDa7/HVdUVIS7774bzzzzDGJjY9GkSRN89dVX2LlzJ959911lu7/97W+YOnUqtmzZ4lKLpzLOz8DNYmJikJCQgKVLl2LevHl46qmnEBMTg6KiIthsNgQHBysVbQMDA9GpUyesWLEC7du3R1hYGO677z639N+qjc9Cz549kZKSgszMTOzZswd9+vSBn58fDh8+jFWrVuEvf/kLnnnmmWodO9UT3hhqRFSRJ554QgQEBIhLly5Vus2IESOEn5+fOHv2rBBCiHPnzonx48eLu+66S/j7+4u7775bDB8+XFkvhBCffvqp6NSpk2jYsGG54bnvvvuuuOuuu4ROpxMPP/yw2LVrV7nhyWVlZeKtt94SLVu2FDqdTnTt2lWsX79eDB8+XLRs2dIlPtRgeLLz0bBhQxEWFibi4+PF5MmTXYZ43qioqEhMnjxZtG3bVvj7+4vw8HDRvXt3MXPmTFFSUuKy74qGPlc0PNnJZrMJACIoKEj89ttv5db//PPPIjExUTRp0kSEh4cLk8kkfvzxx3L7Ky0tFS+++KKIiIgQGo3GZXhqZT+bTZs2CQBCo9GIkydPVnjsR48eFc8//7zQ6/XCz89P3HXXXWLAgAFi9erVyjZ//vOfRbdu3URoaKgIDAwUHTt2FNOmTVN+Nrdy+PBh5Zx8++23LuuKi4vFxIkTRWxsrAgKChKNGzcWsbGxYt68eS7bOYcDb9mypcr3utXwZOew+d27d4thw4aJFi1aCJ1OJyIjI8WAAQPErl27XPa3fft2ERcXJ/z9/V1+zpUNT755mHVln5stW7YIAGLVqlVKW218FoQQYsGCBSIuLk4EBgaKoKAg0blzZ/H666+L3NzcGh071X2c64eIiIhUi31UiIiISLWYqBAREZFqMVEhIiIi1WKiQkRERKrFRIWIiIhUi4kKERERqZbPF3wrKytDbm4ugoKCPFpKmoiIiG6fEAJFRUWIiopyKbB4M59PVHJzc8vN+ElERES+4eTJk7j77rsrXe/ziUpQUBAA+UBvnJaeiIiI1KuwsBDR0dHK93hlfD5Rcd7uCQ4OZqJCRETkY27VbYOdaYmIiEi1mKgQERGRajFRISIiItXy+T4qRCQP8ystLYXD4fB2KFRNWq0WDRs2ZFkFoltgokLk40pKSiBJEi5fvuztUKiGGjVqBIPBAH9/f2+HQqRaTFSIfFhZWRlycnKg1WoRFRUFf39//oXuA4QQKCkpwS+//IKcnBy0a9euyoJXRPUZExUiH1ZSUoKysjJER0ejUaNG3g6HaiAwMBB+fn44fvw4SkpKEBAQ4O2QiFSJKTxRHcC/xn0TzxvRrfF/CREREakWExUiIiK6zmIBMjIAAJIkL0rStXUZGXKDB7k1UcnMzMSDDz6IoKAgREZGYuDAgTh06JDLNleuXMG4cePQrFkzNGnSBIMGDUJ+fr47wyIiqpJGo8Enn3zi7TCIvEOrBVJTgYwMSBIwdeq1RCUjQ27Xaj0ajlsTlW3btmHcuHHYsWMHNm3ahKtXr6JPnz64dOmSss2ECRPw2WefYdWqVdi2bRtyc3Px9NNPuzMsIrrRDX89leOBv56ysrKg1WqRlJRUo9e1atUKc+bMcU9QRPXZlClAerqclNhscpvNJi+np8vrPcito342btzosrxkyRJERkbCbrfjkUceQUFBARYtWoR//vOf+P3vfw8AWLx4Me655x7s2LEDDz30ULl9FhcXo7i4WFkuLCx05yEQ1X3Ov54A119Azr+e0tPd+vaLFi3Ciy++iEWLFiE3NxdRUVFufT8iqpokAVLSFCBXj93WbAAm+dm8AEgywSABBoMHAxIedPjwYQFA7N27VwghxObNmwUA8euvv7ps16JFCzFr1qwK95GWliYAlHsUFBS4O3wi1fntt9/Ezz//LH777bc721F6uhCA/FzRspsUFRWJJk2aiIMHD4ohQ4aIadOmuaxft26deOCBB4ROpxPNmjUTAwcOFEII0bNnz3K/A4SQfz/Exsa67GP27NmiZcuWyvL3338vEhMTRbNmzURwcLB45JFHhN1ud3kNALF27dpaP96b1dr5I6pFaWnyf//KHmlptfM+BQUF1fr+9lhn2rKyMrz88st4+OGHcd999wEA8vLy4O/vj9DQUJdtmzdvjry8vAr3M3nyZBQUFCiPkydPujt0orrvxku9Op3HLvGuXLkSHTt2RIcOHfDHP/4RH3zwAYQQAIANGzbgqaeewuOPP44ffvgBmzdvRrdu3QAAa9aswd1334309HRIkgRJ6el3a0VFRRg+fDi+/fZb7NixA+3atcPjjz+OoqIitxwjka9JSQHsdsButsGGZACADcmwm22w2+X1nuSxgm/jxo3Dvn378O23397RfnQ6HXQ6XS1FRUSKKVOAP/8ZKCkB/P09ch960aJF+OMf/wgA6NevHwoKCrBt2zb06tUL06ZNw9ChQzF16lRl+9jYWABAWFgYtFotgoKCoNfra/SeztvMTgsWLEBoaCi2bduGAQMG3OEREfk+gwEwLMwArKny7R4rYDTHw2gdA0TlebyPikeuqIwfPx7r16/Hli1bcPfddyvter0eJSUluHDhgsv2+fn5Nf7lQ0R3KCPjepJSUlJ5B9tacujQIXz//fcYNmwYAKBhw4YYMmQIFi1aBADYs2cPHn300Vp/3/z8fJhMJrRr1w4hISEIDg7GxYsXceLEiVp/LyKfdGP/NJNJbjOZrl91dfPvhpu59YqKEAIvvvgi1q5di61bt6J169Yu6+Pi4uDn54fNmzdj0KBBAORfXidOnEBCQoI7QyOiG934i2nKlOvLgNv+elq0aBFKS0tdOs8KIaDT6fC3v/0NgYGBNd5ngwYNlFtHTlevXnVZHj58OM6dO4e//OUvaNmyJXQ6HRISElBSUnJ7B0JU1zgcyu8CgwSkpV3rPOv8XeDhWdrdmqiMGzcO//znP/Hpp58iKChI6XcSEhKCwMBAhISEYPTo0XjllVcQFhaG4OBgvPjii0hISKhwxA8RucHNSQpw/dlNyUppaSn+/ve/491330WfPn1c1g0cOBAfffQRunTpgs2bN2PkyJEV7sPf3x+Om35hRkREIC8vD0IIZXLGPXv2uGzz3XffYd68eXj88ccBACdPnsTZs2dr6ciI6oAbShIYDDdVKPDwbR/AzYnK+++/DwDo1auXS/vixYsxYsQIAMDs2bPRoEEDDBo0CMXFxejbty/mzZvnzrCI6EY3/PXkwo1/Pa1fvx6//vorRo8ejZCQEJd1gwYNwqJFizBjxgw8+uijiImJwdChQ1FaWorPP/8cb7zxBgC5jsrXX3+NoUOHQqfTITw8HL169cIvv/yC6dOn45lnnsHGjRvxxRdfIDg4WNl/u3bt8I9//AMPPPAACgsLMXHixNu6ekNEHlI7g4y8p7rDm4jqIl8d3jpgwADx+OOPV7guOztbABA//vij+Pjjj8X9998v/P39RXh4uHj66aeV7bKyskSXLl2ETqcTN/4qe//990V0dLRo3LixeP7558W0adNchifv3r1bPPDAAyIgIEC0a9dOrFq1SrRs2VLMnj1b2QYcnkzkdtX9/tYIcdMNXR9TWFiIkJAQFBQUuPzVRFQfXLlyBTk5OWjdujUCAgK8HQ7VEM8f1WfV/f7mpIRERESkWkxUiIiISLWYqBAREZFqMVEhIiIi1WKiQkRE5IssFqVKrCTJi8q0VxkZNxVA8V1MVIiIiHyRVquUtJckYOrUa4mKs4ijVuvtCGuFxyYlJCIiolp0YwXpXD0AE2CzyZMJemD2c09hokJEROSDJAmQkqYAuXrstmYDMMnP5gVAkgkG6docPT6Ot36IiIh80Pz5QFwcEGc1wYSFAAATFiLOakJcnLy+LmCiQkR1wpIlSxAaGurtMIg8JiUFsNsBu9kGG5IBADYkw262wW6X19cFTFSISFFu5IAbjRgxAhqNptyjX79+t3xtq1atMGfOHJe2IUOG4D//+Y+bor2OCRGphcEAGDdkwGgdA6M5HgBgNMfLyxsy6sRtH4B9VIjoBs6RA3/4g2fubffr1w+LFy92adPpdLe1r8DAQM6CTPWLc3RPejqQZAKsAEwmICpPbgfqRIdaXlEhIq/R6XTQ6/Uuj6ZNm0IIAYvFghYtWkCn0yEqKgovvfQSAKBXr144fvw4JkyYoFyFAcpf6bBYLLj//vvxwQcfoEWLFmjSpAnGjh0Lh8OB6dOnQ6/XIzIyEtOmTXOJadasWejcuTMaN26M6OhojB07FhcvXgQAbN26FSNHjkRBQYHy3pZrtSqKi4vx2muv4a677kLjxo0RHx+PrVu3uv1nSPWYw6GM7jEYgLS0a39gTJkitzsc3o6wVvCKClE9J0nXb/Xs3u36DMi/+Dx9Cfnjjz/G7NmzsXz5ctx7773Iy8vDjz/+CABYs2YNYmNjMWbMGJhMpir3c/ToUXzxxRfYuHEjjh49imeeeQb//e9/0b59e2zbtg3bt2/HqFGjkJiYiPh4+dJ5gwYN8Ne//hWtW7fGf//7X4wdOxavv/465s2bh+7du2POnDlITU3FoUOHAABNmjQBAIwfPx4///wzli9fjqioKKxduxb9+vXD3r170a5dOzf+tKjeuqGgm8FwU323OnAlxYmJClE9N3++fLvnRjd+/6elua/A5fr165Uveqc333wTAQEB0Ov1SExMhJ+fH1q0aIFu3boBAMLCwqDVahEUFAS9Xl/l/svKyvDBBx8gKCgInTp1Qu/evXHo0CF8/vnnaNCgATp06IB33nkHW7ZsURKVl19+WXl9q1at8Oc//xlmsxnz5s2Dv78/QkJCoNFoXN77xIkTWLx4MU6cOIGoqCgAwGuvvYaNGzdi8eLFeOutt2rjx0VULzFRIarnUlLkPimAfCXFdK1mlNEot7nzakrv3r3x/vvvu7SFhYXh0qVLmDNnDtq0aYN+/frh8ccfxxNPPIGGDWv2K6tVq1YICgpSlps3bw6tVosGDRq4tJ05c0ZZ/uqrr5CZmYmDBw+isLAQpaWluHLlCi5fvoxGjRpV+D579+6Fw+FA+/btXdqLi4vRrFmzGsVMRK6YqBDVcxXd2jEarycq7tS4cWO0bdu2XHtYWBgOHTqEr776Cps2bcLYsWMxY8YMbNu2DX5+ftXe/83bajSaCtvKysoAAMeOHcOAAQPwwgsvYNq0aQgLC8O3336L0aNHo6SkpNJE5eLFi9BqtbDb7dDeVLb85itGRFQzTFSISJUCAwPxxBNP4IknnsC4cePQsWNH7N27F0ajEf7+/nC4oaOg3W5HWVkZ3n33XeWqy8qVK122qei9u3btCofDgTNnzqBHjx61HhdRfcZEhYgULiMHPKC4uBh5eXkubQ0bNsT69evhcDgQHx+PRo0a4cMPP0RgYCBatmwJQL6l8/XXX2Po0KHQ6XQIDw+vlXjatm2Lq1ev4r333sMTTzyB7777Dlar1WWbVq1a4eLFi9i8eTNiY2PRqFEjtG/fHs899xyef/55vPvuu+jatSt++eUXbN68GV26dEFSUlKtxEdUH3F4MhEpnCMHPJWobNy4EQaDweXxu9/9DqGhobDZbHj44YfRpUsXfPXVV/jss8+U/h7p6ek4duwYYmJiEBERUWvxxMbGYtasWXjnnXdw3333YdmyZcjMzHTZpnv37jCbzRgyZAgiIiIwffp0AMDixYvx/PPP49VXX0WHDh0wcOBA7Ny5Ey1atKi1+IjqI40QQng7iDtRWFiIkJAQFBQUIDg42NvhEHnUlStXkJOTg9atWyMgIMDb4VAN8fxRfVbd729eUSEiInIXi0WuIIsKpqjIyHDf2P86hIkKERGRu2i1cjn7jAxligpJwvXy9zeNEqPy2JmWiIjIXZwVYlNTgVw9gGuFiqypSvl7qhoTFSIiIjeRJEBKmgLk6rHbmg3AJD+bFwBJJhgkz09R4Wt464eoDvDxPvH1Fs9b3Td/PhAXB8RZTTBhIQDAhIWIs5oQFyevp6oxUSHyYc4qq5cvX/ZyJHQ7nOetJtV2ybekpAB2O2A322BDMgDAhmTYzTbY7fJ6qhpv/RD5MK1Wi9DQUGWumkaNGkGj0Xg5KroVIQQuX76MM2fOIDQ0tFzZfao7DAbAsDBD7pNiXgBYAaM5HkbrGCAqj31UqoGJCpGPc87ie+PEeuQbQkNDbzkDNPk45+ie9HQgyQRYIc/8GZUntwNMVm6BiQqRj9NoNDAYDIiMjMTVq1e9HQ5Vk5+fH6+k1AcOhzK6xyDdMEWFMzlxw5xVdY1bK9N+/fXXmDFjBux2OyRJwtq1azFw4EBl/YgRI7B06VKX1/Tt2xcbN26s9nuwMi0REZHvUUVl2kuXLiE2NhZz586tdJt+/fpBkiTl8dFHH7kzJCIiIvIhbr31079/f/Tv37/KbXQ6XY3u0RYXF6O4uFhZLiwsvO34iIiISN28Pjx569atiIyMRIcOHfDCCy/g3LlzVW6fmZmJkJAQ5REdHe2hSImIiMjTPDZ7skajKddHZfny5WjUqBFat26No0eP4s0330STJk2QlZVVaSeziq6oREdHs48KERGRD6luHxWvjvoZOnSo8u/OnTujS5cuiImJwdatW/Hoo49W+BqdTgedTuepEImIiMiLvH7r50Zt2rRBeHg4jhw54u1QiIiISAVUlaicOnUK586dg4EzNBERERHcnKhcvHgRe/bswZ49ewAAOTk52LNnD06cOIGLFy9i4sSJ2LFjB44dO4bNmzfjySefRNu2bdG3b193hkVEROTKYpGryEKe8dhikZ8ByO0Wi5cCI7cmKrt27ULXrl3RtWtXAMArr7yCrl27IjU1FVqtFj/99BP+8Ic/oH379hg9ejTi4uLwzTffsA8KERF5llYrl7TPyIAkAVOnXktUnCXwWUXYa9zambZXr15VTmP+5ZdfuvPtiYiIqsdZ0j41FcjVAzABNps8meC1EvjkHZzrh4iI6j1JAqSkKUCuHrut2QBM8rN5AZBkgkG6NkcPeZzH6qi4C+f6ISKiO2WxyLd7KpOWxm4qtU0Vc/0QERH5gpQUwG4H7GYbbEgGANiQDLvZBrtdXk/ewUSFiIjqPYMBMG7IgNE6BkZzPADAaI6Xlzdk8LaPF7GPChERkXN0T3o6kGQCrABMJiAqT24H2KHWS5ioEBERORzK6B6DJPdJMRhwPTlxOLwaXn3GzrRERETkcexMS0RERD6PiQoRERGpFhMVIiIiUi0mKkRERKRaTFSIiIhItZioEBERkWoxUSEiIiLVYqJCRES+wWKRK8hCnu3YYpGfAcjtnDWwTmKiQkREvkGrlcvZZ2RAkuTZjiUJ18vfa7XejpDcgCX0iYjINzjL2aemArl6ACbAZgOsqUr5e6p7mKgQEZFPkCRASpoC5Oqx25oNwCQ/mxcASSYYJHCW4zqIc/0QEZFPsFjk2z2VSUtjNxVfwrl+iIioTklJAex2wG62wYZkAIANybCbbbDb5fVU9zBRISIin2AwAMYNGTBax8BojgcAGM3x8vKGDN72qaPYR4WIiHyDc3RPejqQZAKsAEwmICpPbgfYobYOYqJCRES+weFQRvcYJLlPisGA68mJw+HV8Mg92JmWiIiIPI6daYmIiMjnMVEhIiIi1WKiQkRERKrFRIWIiIhUi4kKERERqRYTFSIiIlItJipERFS7LBa5OBvkiQQtFvkZgNzOCXmoBtyaqHz99dd44oknEBUVBY1Gg08++cRlvRACqampMBgMCAwMRGJiIg4fPuzOkIiIyN20WrlSbEYGJEmeSFCScL2yrFbr7QjJh7g1Ubl06RJiY2Mxd+7cCtdPnz4df/3rX2G1WpGdnY3GjRujb9++uHLlijvDIiIid5oyRa4gm5oK2Gxym812vfw9y9xTDbi1hH7//v3Rv3//CtcJITBnzhz8v//3//Dkk08CAP7+97+jefPm+OSTTzB06NAKX1dcXIzi4mJlubCwsPYDJyKi2yZJgJQ0BcjVY7c1G4BJfjYvAJJMMEjgBIJUbV7ro5KTk4O8vDwkJiYqbSEhIYiPj0dWVlalr8vMzERISIjyiI6O9kS4RERUTfPnA3FxQJzVBBMWAgBMWIg4qwlxcfJ6ouryWqKSl5cHAGjevLlLe/PmzZV1FZk8eTIKCgqUx8mTJ90aJxER1UxKCmC3A3azDTYkAwBsSIbdbIPdLq8nqi6fmz1Zp9NBp9N5OwwiIqqEwQAYFmYA1lT5do8VMJrjYbSOAaLy2EeFasRrV1T0ej0AID8/36U9Pz9fWUdERD7IObonPR0wmeQ2k+l6B9trQ5eJqsNriUrr1q2h1+uxefNmpa2wsBDZ2dlISEjwVlhERHSnHA5ldI/BAKSlXes86xwN5HB4O0LyIW699XPx4kUcOXJEWc7JycGePXsQFhaGFi1a4OWXX8af//xntGvXDq1bt8aUKVMQFRWFgQMHujMsIiJypxsKuhkMN9V3420fqiG3Jiq7du1C7969leVXXnkFADB8+HAsWbIEr7/+Oi5duoQxY8bgwoUL+N3vfoeNGzciICDAnWERERGRj9AIIYS3g7gThYWFCAkJQUFBAYKDg70dDhEREVVDdb+/OdcPERERqRYTFSIiIlItJipERESkWkxUiIiISLWYqBAREZFqMVEhIqqPLBalQqwkyYuSdG1dRsZNxU+IvIeJChFRfaTVKuXsJQmYOvVaouIsf6/VejtCIgA+OCkhERHVAmeF2NRUIFcPwATYbPJEgtfK3xOpARMVIqJ6SJIAKWkKkKvHbms2AJP8bF4AJJlgkK7Nz0PkZaxMS0RUD1ks8u2eyqSlsZsKuRcr0xIRUaVSUgC7HbCbbbAhGQBgQzLsZhvsdnk9kRowUSEiqocMBsC4IQNG6xgYzfEAAKM5Xl7ekMHbPqQa7KNCRFQfOUf3pKcDSSbACsBkAqLy5HaAHWpJFZioEBHVRw6HMrrHIMl9UgwGXE9OHA6vhkfkxM60RERE5HHsTEtEREQ+j4kKERERqRYTFSIiIlItJipERESkWkxUiIiISLWYqBAREZFqMVEhIiIi1WKiQkSkZhaLXEUW8ozHFov8DEBu58yBVMcxUSEiUjOtVi5pn5EBSZJnPJYkXC+Br9V6O0Iit2IJfSIiNXOWtE9NBXL1AEyAzQZYU5US+ER1GRMVIiIVkyRASpoC5Oqx25oNwCQ/mxcASSYYJHCmY6rTONcPEZGKWSzy7Z7KpKWxmwr5Js71Q0RUB6SkAHY7YDfbYEMyAMCGZNjNNtjt8nqiuoyJChGRihkMgHFDBozWMTCa4wEARnO8vLwhg7d9qM5jHxUiIjVzju5JTweSTIAVgMkEROXJ7QA71FKdxkSFiEjNHA5ldI9BkvukGAy4npw4HF4Nj8jdvN6Z1mKxYOpNPcU6dOiAgwcPVuv17ExLRETke6r7/a2KKyr33nsvvvrqK2W5YUNVhEVERERepoqMoGHDhtDr9dXatri4GMXFxcpyYWGhu8IiIiIiL1PFqJ/Dhw8jKioKbdq0wXPPPYcTJ05Uum1mZiZCQkKUR3R0tAcjJSIiIk/yeh+VL774AhcvXkSHDh0gSRKmTp2K06dPY9++fQgKCiq3fUVXVKKjo9lHhYiIyIdUt4+K1xOVm124cAEtW7bErFmzMHr06Ftuz860REREvsdnK9OGhoaiffv2OHLkiLdDISIiIi9TXaJy8eJFHD16FAaWWyQiIqr3vJ6ovPbaa9i2bRuOHTuG7du346mnnoJWq8WwYcO8HRoRUfVYLHIFWcizHVss8jMAuZ2zBhLdNq8nKqdOncKwYcPQoUMHPPvss2jWrBl27NiBiIgIb4dGRFQ9Wq1czj4jA5Ikz3YsSbhe/l6r9XaERD7L63VUli9f7u0QiIjujLOcfWoqkKsHYAJsNsCaqpS/J6Lb4/VEhYjI10kSICVNAXL12G3NBmCSn80LgCQTDBI4yzHRbVLd8OSa4vBkIvI2i0W+3VOZtDR2UyG6mc8OTyYi8jUpKYDdDtjNNtiQDACwIRl2sw12u7yeiG4PExUiojtkMADGDRkwWsfAaI4HABjN8fLyhgze9iG6A+yjQkR0p5yje9LTgSQTYAVgMgFReXI7wA61RLeJiQoR0Z1yOJTRPQZJ7pNiMOB6cuJweDU8Il/GzrRERETkcexMS0RERD6PiQoRERGpFhMVIiIiUi0mKkRERKRaTFSIiIhItZioEBERkWoxUSEiIiLVYqJCRPWHxSJXkYU847HFIj8DkNs5cyCR6jBRIaL6Q6uVS9pnZECS5BmPJQnXS+Brtd6OkIhuwhL6RFR/OEvap6YCuXoAJsBmA6ypSgl8IlIXJipEVG9IEiAlTQFy9dhtzQZgkp/NC4AkEwwSONMxkcpwrh8iqjcsFvl2T2XS0thNhchTONcPEdFNUlIAux2wm22wIRkAYEMy7GYb7HZ5PRGpCxMVIqo3DAbAuCEDRusYGM3xAACjOV5e3pDB2z5EKsQ+KkRUfzhH96SnA0kmwArAZAKi8uR2gB1qiVSGiQoR1R8OhzK6xyDJfVIMBlxPThwOr4ZHROWxMy0RERF5HDvTEhERkc9jokJERESqxUSFiIiIVIuJChEREakWExUiIiJSLSYqREREpFpMVIhIXSwWuTAb5EkELRb5GYDczsl4iOoVVSQqc+fORatWrRAQEID4+Hh8//333g6JiLxFq5WrxGZkQJLkSQQlCderymq13o6QiDzI65VpV6xYgVdeeQVWqxXx8fGYM2cO+vbti0OHDiEyMtLb4RGRpzmrxKamArl6ACbAZgOsqUpVWSKqP7xemTY+Ph4PPvgg/va3vwEAysrKEB0djRdffBGTJk0qt31xcTGKi4uV5cLCQkRHR7MyLVEdIUnXrqDYbNhtzYYJC2FDsjyJoMkEgwGcPJCoDvCJyrQlJSWw2+1ITExU2ho0aIDExERkZWVV+JrMzEyEhIQoj+joaE+FS0QeMH8+EBcHxFlNMGEhAMCEhYizmhAXJ68novrDq4nK2bNn4XA40Lx5c5f25s2bIy8vr8LXTJ48GQUFBcrj5MmTngiViDwkJQWw2wG72QYbkgEANiTDbrbBbpfXE1H94fU+KjWl0+mg0+m8HQYRuYnBABgWZsh9UswLACtgNMfDaB0DROWxjwpRPePVRCU8PBxarRb5+fku7fn5+dDr9V6Kioi8yjm6Jz0dSDIBVgAmk5ykpKbK2zBZIao3vHrrx9/fH3Fxcdi8ebPSVlZWhs2bNyMhIcGLkRGR1zgcyugegwFIS7vWeXbKFLnd4fB2hETkQV4f9bNixQoMHz4c8+fPR7du3TBnzhysXLkSBw8eLNd3pSLV7TVMRERE6lHd72+v91EZMmQIfvnlF6SmpiIvLw/3338/Nm7cWK0khYiIiOo2r19RuVO8okJEROR7fKKOChEREVFVmKgQERGRajFRISIiItViokJERESqxUSFiIiIVIuJChHVnMUiV5CFPNOxxXJtxmNAbrdYvBQYEdU1TFSIqOa0WrmcfUYGJAmYOvVaouIsf6/VejtCIqojvF7wjYh8kHOundRUIFcPwATYbPJEgtfK3xMR1QYmKkRUY5IESElTgFw9dluzAZjkZ/MCIMkEg3Rtfh4iojvEyrREVGMWi3y7pzJpaeymQkRVY2VaInKblBTAbgfsZhtsSAYA2JAMu9kGu11eT0RUG5ioEFGNGQyAcUMGjNYxMJrjAQBGc7y8vCGDt32IqNawjwoR1ZxzdE96OpBkAqwATCYgKk9uB9ihlohqBRMVIqo5h0MZ3WOQ5D4pBgOuJycOh1fDI6K6g51piYiIyOPYmZaIiIh8HhMVIiIiUi0mKkRERKRaTFSIiIhItZioEBERkWoxUSEiIiLVYqJCREREqsVEhagus1jkKrKQZzy2WORnAHI7Zw4kIpVjokJUl2m1ckn7jAxIkjzjsSThegl8rdbbERIRVYkl9InqMmdJ+9RUIFcPwATYbIA1VSmBT0SkZkxUiOowSQKkpClArh67rdkATPKzeQGQZIJBAmc6JiJV41w/RHWYxSLf7qlMWhq7qRCRd3CuHyJCSgpgtwN2sw02JAMAbEiG3WyD3S6vJyJSMyYqRHWYwQAYN2TAaB0DozkeAGA0x8vLGzJ424eIVI99VIjqMufonvR0IMkEWAGYTEBUntwOsEMtEakaExWiuszhUEb3GCS5T4rBgOvJicPh1fCIiG7Fq51pW7VqhePHj7u0ZWZmYtKkSdXeBzvTEhER+Z7qfn97/YpKeno6TCaTshwUFOTFaIiIiEhNvJ6oBAUFQa/XV3v74uJiFBcXK8uFhYXuCIuIiIhUwOujft5++200a9YMXbt2xYwZM1BaWlrl9pmZmQgJCVEe0dHRHoqUiIiIPM2rfVRmzZoFo9GIsLAwbN++HZMnT8bIkSMxa9asSl9T0RWV6Oho9lEhIiLyIdXto1LricqkSZPwzjvvVLnNgQMH0LFjx3LtH3zwAVJSUnDx4kXodLpqvR870xIREfkeryUqv/zyC86dO1flNm3atIG/v3+59v379+O+++7DwYMH0aFDh2q9HxMVIiIi3+O1UT8RERGIiIi4rdfu2bMHDRo0QGRkZC1HRURERL7Ia6N+srKykJ2djd69eyMoKAhZWVmYMGEC/vjHP6Jp06beCovI8ywWQKsFpkyBJAHz58tz8BgMkCvLOhycOZCI6i2vjfrR6XRYvnw5evbsiXvvvRfTpk3DhAkTsGDBAm+FROQdWq1czj4jA5Ikz3YsSbhe/l6r9XaERERe47UrKkajETt27PDW2xOph7OcfWoqkKsHYAJsNsCaqpS/JyKqr7xe8I2ovpMkQEqaAuTqsduaDcAkP5sXAEkmGCRwlmMiqre8WkelNnDUD/k6i0W+3VOZtDR2USGiuqe6399er0xLVN+lpAB2O2A322BDMgDAhmTYzTbY7fJ6IqL6iokKkZcZDIBxQwaM1jEwmuMBAEZzvLy8IYO3fYioXmMfFSJvc47uSU8HkkyAFYDJBETlye0AO9QSUb3FRIXI2xwOZXSPQZL7pBgMuJ6cOBxeDY+IyJvYmZaIiIg8jp1piYiIyOcxUSEiIiLVYqJCREREqsVEhYiIiFSLiQoRERGpFhMVIiIiUi0mKkRERKRaTFSIqstikavIQp7x2GKRnwHI7Zw5kIio1jFRIaourVYuaZ+RAUmSZzyWJFwvga/VejtCIqI6hyX0iarLWdI+NRXI1QMwATYbYE1VSuATEVHtYqJCVE2SBEhJU4BcPXZbswGY5GfzAiDJBIMEznRMRFTLONcPUTVZLPLtnsqkpbGbChFRdXGuH6JalpIC2O2A3WyDDckAABuSYTfbYLfL64mIqHYxUSGqJoMBMG7IgNE6BkZzPADAaI6Xlzdk8LYPEZEbsI8KUXU5R/ekpwNJJsAKwGQCovLkdoAdaomIahkTFaLqcjiU0T0GSe6TYjDgenLicHg1PCKiuoidaYmIiMjj2JmWiIiIfB4TFSIiIlItJipERESkWkxUiIiISLWYqBAREZFqMVEhIiIi1WKiQnWLxSIXZoM8iaDFIj8DkNs5GQ8RkU9xW6Iybdo0dO/eHY0aNUJoaGiF25w4cQJJSUlo1KgRIiMjMXHiRJSWlrorJKoPtFq5SmxGBiRJnkRQknC9qqxW6+0IiYioBtxWmbakpASDBw9GQkICFi1aVG69w+FAUlIS9Ho9tm/fDkmS8Pzzz8PPzw9vvfWWu8Kius5ZJTY1FcjVAzABNhtgTVWqyhIRke9we2XaJUuW4OWXX8aFCxdc2r/44gsMGDAAubm5aN68OQDAarXijTfewC+//AJ/f/8K91dcXIzi4mJlubCwENHR0axMSwDkqyeSBMBmw25rNkxYCBuS5UkETSYYDODkgUREKqD6yrRZWVno3LmzkqQAQN++fVFYWIj9+/dX+rrMzEyEhIQoj+joaE+ESz5i/nwgLg6Is5pgwkIAgAkLEWc1IS5OXk9ERL7Da4lKXl6eS5ICQFnOy8ur9HWTJ09GQUGB8jh58qRb4yTfkpIC2O2A3WyDDckAABuSYTfbYLfL64mIyHfUKFGZNGkSNBpNlY+DBw+6K1YAgE6nQ3BwsMuDyMlgAIwbMmC0jpFv9wAwmuPl5Q0ZvO1DRORjatSZ9tVXX8WIESOq3KZNmzbV2pder8f333/v0pafn6+sI7otztE96elAkgmwAjCZgKg8uR1gh1oiIh9So0QlIiICERERtfLGCQkJmDZtGs6cOYPIyEgAwKZNmxAcHIxOnTrVyntQPeRwKKN7DBKQlnat86wzOXE4vBoeERHVjNtG/Zw4cQLnz5/HunXrMGPGDHzzzTcAgLZt26JJkyZwOBy4//77ERUVhenTpyMvLw//+7//i+Tk5BoNT65ur2EiIiJSj+p+f7stURkxYgSWLl1arn3Lli3o1asXAOD48eN44YUXsHXrVjRu3BjDhw/H22+/jYYNq3+hh4kKERGR7/F6ouIpTFSIiIh8j+rrqBARERHdChMVIiIiUi0mKkRERKRaTFSIiIhItZioEBERkWoxUSHvsFjkKrKQZzu2WK7NegzI7RaLlwIjIiI1YaJC3qHVyiXtMzIgScDUqdcSFWcJfK3W2xESEZEK1KiEPlGtcZa0T00FcvUATIDNBlhTlRL4RERETFTIKyQJkJKmALl67LZmAzDJz+YFQJIJBgmc6ZiIiFiZlrzDYpFv91QmLY3dVIiI6jJWpiVVS0kB7HbAbrbBhmQAgA3JsJttsNvl9URERExUyCsMBsC4IQNG6xgYzfEAAKM5Xl7ekMHbPkREBIB9VMhbnKN70tOBJBNgBWAyAVF5cjvADrVERMREhbzE4VBG9xgkuU+KwYDryYnD4dXwiIhIHdiZloiIiDyOnWmJiIjI5zFRISIiItViokJERESqxUSFiIiIVIuJChEREakWExUiIiJSLSYqREREpFpMVKhyFotcQRbybMcWi/wMQG7nrIFERORmTFSoclqtXM4+IwOSJM92LEm4Xv5eq/V2hEREVMexhD5VzlnOPjUVyNUDMAE2G2BNVcrfExERuRMTFaqUJAFS0hQgV4/d1mwAJvnZvABIMsEggbMcExGRW3GuH6qUxSLf7qlMWhq7qRAR0e3hXD90x1JSALsdsJttsCEZAGBDMuxmG+x2eT0REZE7MVGhShkMgHFDBozWMTCa4wEARnO8vLwhg7d9iIjI7dhHhSrnHN2Tng4kmQArAJMJiMqT2wF2qCUiIrdiokKVcziU0T0GSe6TYjDgenLicHg1PCIiqvvc1pl22rRp2LBhA/bs2QN/f39cuHCh/JtrNOXaPvroIwwdOrTa78POtERERL6nut/fbruiUlJSgsGDByMhIQGLFi2qdLvFixejX79+ynJoaKi7QiIiIiIf47ZEZeq1ca1LliypcrvQ0FDo9fpq77e4uBjFxcXKcmFh4W3FR0REROrn9VE/48aNQ3h4OLp164YPPvgAt7oTlZmZiZCQEOURHR3toUiJiIjI07yaqKSnp2PlypXYtGkTBg0ahLFjx+K9996r8jWTJ09GQUGB8jh58qSHoiUiIiJPq9Gtn0mTJuGdd96pcpsDBw6gY8eO1drflBuGtnbt2hWXLl3CjBkz8NJLL1X6Gp1OB51OV72AiYiIyKfVKFF59dVXMWLEiCq3adOmzW0HEx8fj4yMDBQXFzMZISIiopolKhEREYiIiHBXLNizZw+aNm3KJIWIiIgAuHHUz4kTJ3D+/HmcOHECDocDe/bsAQC0bdsWTZo0wWeffYb8/Hw89NBDCAgIwKZNm/DWW2/htddec1dIdZfFAmi1wJQpkCRg/nx5Hh6DAXJ1WYeDswcSEZFPcluikpqaiqVLlyrLXbt2BQBs2bIFvXr1gp+fH+bOnYsJEyZACIG2bdti1qxZMJlM7gqp7tJqlZL2UtIUTJ0K/OEPgGHhDSXwiYiIfJDbKtN6CivTXnNtXp7d5gWIs5pgN9tgtI5RSuATERGpidcr05LnSJJ8JQW5euy2ZgMwyc/mBUCSCQYJnOmYiIh8Eq+o1AEWC3CtEHCF0tLYRYWIiNSlut/fXq9MS3cuJQWw2wG72QYbkgEANiTDbrbBbpfXExER+SImKnWAwQAYN2TAaB0DozkeAGA0x8vLGzJ424eIiHwW+6jUBRk3jO5JMgFWACYTEJWnjAZih1oiIvJFTFTqAodDGd1jkOQ+KQYDricnDodXwyMiIrpd7ExLREREHsfOtEREROTzmKgQERGRajFRISIiItViokJERESqxUSFiIiIVIuJChEREakWExUiIiJSLSYqnmKxyBVkIc92bLHIzwDkds4aSEREVA4TFU/RauVy9hkZkCR5tmNJwvXy91qttyMkIiJSHZbQ9xRnOfvUVCBXD8AE2GyANVUpf09ERESumKh4iCQBUtIUIFeP3dZsACb52bwASDLBIIGzHBMREd2Ec/14iMUi3+6pTFoau6kQEVH9wbl+VCYlBbDbAbvZBhuSAQA2JMNutsFul9cTERGRKyYqHmIwAMYNGTBax8BojgcAGM3x8vKGDN72ISIiqgD7qHiKc3RPejqQZAKsAEwmICpPbgfYoZaIiOgmTFQ8xeFQRvcYJLlPisGA68mJw+HV8IiIiNSInWmJiIjI49iZloiIiHweExUiIiJSLSYqREREpFpMVIiIiEi1mKgQERGRajFRISIiItVionIzi0UuzgZ5IkGLRX4GILdzQh4iIiKPcVuicuzYMYwePRqtW7dGYGAgYmJikJaWhpKSEpftfvrpJ/To0QMBAQGIjo7G9OnT3RVS9Wi1cqXYjAxIkjyRoCThemVZrda78REREdUjbqtMe/DgQZSVlWH+/Plo27Yt9u3bB5PJhEuXLmHmzJkA5GIvffr0QWJiIqxWK/bu3YtRo0YhNDQUY8aMcVdoVXNWik1NBXL1AEyAzQZYU5XKskREROQZHq1MO2PGDLz//vv473//CwB4//338X//93/Iy8uDv78/AGDSpEn45JNPcPDgwQr3UVxcjOLiYmW5sLAQ0dHRtVaZVpKuXUGx2bDbmg0TFsKGZHkiQZMJBgM4gSAREdEdUmVl2oKCAoSFhSnLWVlZeOSRR5QkBQD69u2LQ4cO4ddff61wH5mZmQgJCVEe0dHRtRrj/PlAXBwQZzXBhIUAABMWIs5qQlycvJ6IiIg8w2OJypEjR/Dee+8hJSVFacvLy0Pz5s1dtnMu5+XlVbifyZMno6CgQHmcPHmyVuNMSQHsdsButsGGZACADcmwm22w2+X1RERE5Bk1TlQmTZoEjUZT5ePm2zanT59Gv379MHjwYJhMpjsKWKfTITg42OVRmwwGwLghA0brGPl2DwCjOV5e3pDB2z5EREQeVOPOtK+++ipGjBhR5TZt2rRR/p2bm4vevXuje/fuWLBggct2er0e+fn5Lm3OZb1eX9PQaodzdE96OpBkAqwATCYgKk9uB9ihloiIyENqnKhEREQgIiKiWtuePn0avXv3RlxcHBYvXowGDVwv4CQkJOD//u//cPXqVfj5+QEANm3ahA4dOqBp06Y1Da12OBzK6B6DBKSlXes860xOHA7vxEVERFQPuW3Uz+nTp9GrVy+0bNkSS5cuhfaG+iPOqyUFBQXo0KED+vTpgzfeeAP79u3DqFGjMHv27GoPT65ur2EiIiJSj+p+f7utjsqmTZtw5MgRHDlyBHfffbfLOmduFBISgn/9618YN24c4uLiEB4ejtTUVO/VUCEiIiJV8WgdFXfgFRUiIiLfo8o6KkREREQ1wUSFiIiIVIuJChEREakWExUiIiJSLSYqREREpFpMVIiIiEi1mKgQERGRajFRISIiItVyW2VaT3HWqyssLPRyJERERFRdzu/tW9Wd9flEpaioCAAQHR3t5UiIiIiopoqKihASElLpep8voV9WVobc3FwEBQVBo9HU6r4LCwsRHR2NkydP1sny/Dw+31fXj5HH5/vq+jHy+G6fEAJFRUWIiopCgwaV90Tx+SsqDRo0KDfpYW0LDg6ukx9AJx6f76vrx8jj8311/Rh5fLenqispTuxMS0RERKrFRIWIiIhUi4lKFXQ6HdLS0qDT6bwdilvw+HxfXT9GHp/vq+vHyONzP5/vTEtERER1F6+oEBERkWoxUSEiIiLVYqJCREREqsVEhYiIiFSLiQoRERGpVr1OVKZNm4bu3bujUaNGCA0NrXCbEydOICkpCY0aNUJkZCQmTpyI0tLSKvd7/vx5PPfccwgODkZoaChGjx6NixcvuuEIambr1q3QaDQVPnbu3Fnp63r16lVue7PZ7MHIq69Vq1blYn377berfM2VK1cwbtw4NGvWDE2aNMGgQYOQn5/voYir79ixYxg9ejRat26NwMBAxMTEIC0tDSUlJVW+Tu3nb+7cuWjVqhUCAgIQHx+P77//vsrtV61ahY4dOyIgIACdO3fG559/7qFIayYzMxMPPvgggoKCEBkZiYEDB+LQoUNVvmbJkiXlzlVAQICHIq45i8VSLt6OHTtW+RpfOX9Axb9PNBoNxo0bV+H2vnD+vv76azzxxBOIioqCRqPBJ5984rJeCIHU1FQYDAYEBgYiMTERhw8fvuV+a/r/uCbqdaJSUlKCwYMH44UXXqhwvcPhQFJSEkpKSrB9+3YsXboUS5YsQWpqapX7fe6557B//35s2rQJ69evx9dff40xY8a44xBqpHv37pAkyeWRnJyM1q1b44EHHqjytSaTyeV106dP91DUNZeenu4S64svvljl9hMmTMBnn32GVatWYdu2bcjNzcXTTz/toWir7+DBgygrK8P8+fOxf/9+zJ49G1arFW+++eYtX6vW87dixQq88sorSEtLw+7duxEbG4u+ffvizJkzFW6/fft2DBs2DKNHj8YPP/yAgQMHYuDAgdi3b5+HI7+1bdu2Ydy4cdixYwc2bdqEq1evok+fPrh06VKVrwsODnY5V8ePH/dQxLfn3nvvdYn322+/rXRbXzp/ALBz506XY9u0aRMAYPDgwZW+Ru3n79KlS4iNjcXcuXMrXD99+nT89a9/hdVqRXZ2Nho3boy+ffviypUrle6zpv+Pa0yQWLx4sQgJCSnX/vnnn4sGDRqIvLw8pe39998XwcHBori4uMJ9/fzzzwKA2Llzp9L2xRdfCI1GI06fPl3rsd+JkpISERERIdLT06vcrmfPnuJPf/qTZ4K6Qy1bthSzZ8+u9vYXLlwQfn5+YtWqVUrbgQMHBACRlZXlhghr1/Tp00Xr1q2r3EbN569bt25i3LhxyrLD4RBRUVEiMzOzwu2fffZZkZSU5NIWHx8vUlJS3BpnbThz5owAILZt21bpNpX9LlKrtLQ0ERsbW+3tffn8CSHEn/70JxETEyPKysoqXO9r5w+AWLt2rbJcVlYm9Hq9mDFjhtJ24cIFodPpxEcffVTpfmr6/7im6vUVlVvJyspC586d0bx5c6Wtb9++KCwsxP79+yt9TWhoqMsVisTERDRo0ADZ2dluj7km1q1bh3PnzmHkyJG33HbZsmUIDw/Hfffdh8mTJ+Py5cseiPD2vP3222jWrBm6du2KGTNmVHmrzm634+rVq0hMTFTaOnbsiBYtWiArK8sT4d6RgoIChIWF3XI7NZ6/kpIS2O12l599gwYNkJiYWOnPPisry2V7QP4/6SvnCsAtz9fFixfRsmVLREdH48knn6z0d41aHD58GFFRUWjTpg2ee+45nDhxotJtffn8lZSU4MMPP8SoUaOg0Wgq3c7Xzt+NcnJykJeX53KOQkJCEB8fX+k5up3/xzXl87Mnu1NeXp5LkgJAWc7Ly6v0NZGRkS5tDRs2RFhYWKWv8ZZFixahb9++t5x9+n/+53/QsmVLREVF4aeffsIbb7yBQ4cOYc2aNR6KtPpeeuklGI1GhIWFYfv27Zg8eTIkScKsWbMq3D4vLw/+/v7l+ig1b95cdefrZkeOHMF7772HmTNnVrmdWs/f2bNn4XA4Kvw/dvDgwQpfU9n/SbWfq7KyMrz88st4+OGHcd9991W6XYcOHfDBBx+gS5cuKCgowMyZM9G9e3fs37/f7bPE3474+HgsWbIEHTp0gCRJmDp1Knr06IF9+/YhKCio3Pa+ev4A4JNPPsGFCxcwYsSISrfxtfN3M+d5qMk5up3/xzVV5xKVSZMm4Z133qlymwMHDtyyw5cvuZ1jPnXqFL788kusXLnylvu/sX9N586dYTAY8Oijj+Lo0aOIiYm5/cCrqSbH98orryhtXbp0gb+/P1JSUpCZmanauThu5/ydPn0a/fr1w+DBg2Eymap8rbfPHwHjxo3Dvn37quy/AQAJCQlISEhQlrt374577rkH8+fPR0ZGhrvDrLH+/fsr/+7SpQvi4+PRsmVLrFy5EqNHj/ZiZLVv0aJF6N+/P6KioirdxtfOn6+oc4nKq6++WmXGCwBt2rSp1r70en25nsvO0SB6vb7S19zcgai0tBTnz5+v9DV36naOefHixWjWrBn+8Ic/1Pj94uPjAch/0Xvii+5Ozml8fDxKS0tx7NgxdOjQodx6vV6PkpISXLhwweWqSn5+vtvO181qeny5ubno3bs3unfvjgULFtT4/Tx9/ioTHh4OrVZbboRVVT97vV5fo+3VYPz48Uqn+pr+Ve3n54euXbviyJEjboqudoWGhqJ9+/aVxuuL5w8Ajh8/jq+++qrGVyF97fw5z0N+fj4MBoPSnp+fj/vvv7/C19zO/+Maq5WeLj7uVp1p8/Pzlbb58+eL4OBgceXKlQr35exMu2vXLqXtyy+/VFVn2rKyMtG6dWvx6quv3tbrv/32WwFA/Pjjj7UcWe378MMPRYMGDcT58+crXO/sTLt69Wql7eDBg6rtTHvq1CnRrl07MXToUFFaWnpb+1DT+evWrZsYP368suxwOMRdd91VZWfaAQMGuLQlJCSosjNmWVmZGDdunIiKihL/+c9/bmsfpaWlokOHDmLChAm1HJ17FBUViaZNm4q//OUvFa73pfN3o7S0NKHX68XVq1dr9Dq1nz9U0pl25syZSltBQUG1OtPW5P9xjeOslb34qOPHj4sffvhBTJ06VTRp0kT88MMP4ocffhBFRUVCCPlDdt9994k+ffqIPXv2iI0bN4qIiAgxefJkZR/Z2dmiQ4cO4tSpU0pbv379RNeuXUV2drb49ttvRbt27cSwYcM8fnyV+eqrrwQAceDAgXLrTp06JTp06CCys7OFEEIcOXJEpKeni127domcnBzx6aefijZt2ohHHnnE02Hf0vbt28Xs2bPFnj17xNGjR8WHH34oIiIixPPPP69sc/PxCSGE2WwWLVq0EP/+97/Frl27REJCgkhISPDGIVTp1KlTom3btuLRRx8Vp06dEpIkKY8bt/Gl87d8+XKh0+nEkiVLxM8//yzGjBkjQkNDlZF2//u//ysmTZqkbP/dd9+Jhg0bipkzZ4oDBw6ItLQ04efnJ/bu3eutQ6jUCy+8IEJCQsTWrVtdztXly5eVbW4+vqlTp4ovv/xSHD16VNjtdjF06FAREBAg9u/f741DuKVXX31VbN26VeTk5IjvvvtOJCYmivDwcHHmzBkhhG+fPyeHwyFatGgh3njjjXLrfPH8FRUVKd91AMSsWbPEDz/8II4fPy6EEOLtt98WoaGh4tNPPxU//fSTePLJJ0Xr1q3Fb7/9puzj97//vXjvvfeU5Vv9P75T9TpRGT58uABQ7rFlyxZlm2PHjon+/fuLwMBAER4eLl599VWXrHrLli0CgMjJyVHazp07J4YNGyaaNGkigoODxciRI5XkRw2GDRsmunfvXuG6nJwcl5/BiRMnxCOPPCLCwsKETqcTbdu2FRMnThQFBQUejLh67Ha7iI+PFyEhISIgIEDcc8894q233nK5+nXz8QkhxG+//SbGjh0rmjZtKho1aiSeeuoply9/tVi8eHGFn9cbL4z64vl77733RIsWLYS/v7/o1q2b2LFjh7KuZ8+eYvjw4S7br1y5UrRv3174+/uLe++9V2zYsMHDEVdPZedq8eLFyjY3H9/LL7+s/CyaN28uHn/8cbF7927PB19NQ4YMEQaDQfj7+4u77rpLDBkyRBw5ckRZ78vnz+nLL78UAMShQ4fKrfPF8+f8zrr54TyOsrIyMWXKFNG8eXOh0+nEo48+Wu7YW7ZsKdLS0lzaqvp/fKc0QghROzeRiIiIiGoX66gQERGRajFRISIiItViokJERESqxUSFiIiIVIuJChEREakWExUiIiJSLSYqREREpFpMVIiIiEi1mKgQERGRajFRISIiItViokJERESq9f8BxWYSG0T+lwcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Derivatives\n",
        "\n",
        "`f` is a function of many variables, and it has multiple partial derivatives, each indicating how f changes when we make small changes to one input variable keeping others fixed."
      ],
      "metadata": {
        "id": "n4SHp-I42SnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = [random.uniform(-10, 10) for i in range(3)]\n",
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHUo8sW_3OpN",
        "outputId": "617cc91c-46f4-413b-fdd2-d01c004809c3"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-5.2539215111695015, -1.5989748008581923, -5.985600104689148]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "dJBqXHKFBWkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f(x, y) = x^2 + 2xy + y^3\n",
        "def my_fun(v:Vector) -> float:\n",
        "  x,y = v[0], v[1]\n",
        "  return x**2 + 2*x*y + y**3\n",
        "\n",
        "point = [1.0, 2.0] # evaluate at point (1,2)\n",
        "h = 0.000001\n",
        "\n",
        "pd_x = partial_difference_quotient(my_fun, point, i=0, h=h)\n",
        "print(f\"df/dx, at(1,2) ~ {pd_x: 4f}\")\n",
        "\n",
        "pd_y = partial_difference_quotient(my_fun, point, i=1, h=h)\n",
        "print(f\"df/dy, at(1,2) ~ {pd_y: 4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE-2fjl86lj1",
        "outputId": "93d67f8d-e75f-4884-d4cc-eb30b1462d89"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w [1.000001, 2.0]\n",
            "df/dx, at(1,2) ~  6.000001\n",
            "w [1.0, 2.000001]\n",
            "df/dy, at(1,2) ~  14.000006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Using the Gradient\" example\n",
        "\n",
        "# pick a random starting point\n",
        "v = [random.uniform(-10, 10) for i in range(3)]"
      ],
      "metadata": {
        "id": "gc3f6fPx8wOI"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQsztjh780uJ",
        "outputId": "723048d9-6863-4441-dbdf-ce535754b398"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.477851124277649, 1.906221283715066, 6.129276917687147]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Gradient\n",
        "\n",
        "Example code from book, page 101"
      ],
      "metadata": {
        "id": "nZItW7d2BgtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "    grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
        "    v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n",
        "    # print(epoch, v)\n",
        "\n",
        "assert distance(v, [0, 0, 0]) < 0.001    # v should be close to 0"
      ],
      "metadata": {
        "id": "fGVN2jS38_4u"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VejodpHhNj1n",
        "outputId": "b81e1493-27ec-42be-f4d6-e9c8ec13b4dc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-50, -995),\n",
              " (-49, -975),\n",
              " (-48, -955),\n",
              " (-47, -935),\n",
              " (-46, -915),\n",
              " (-45, -895),\n",
              " (-44, -875),\n",
              " (-43, -855),\n",
              " (-42, -835),\n",
              " (-41, -815),\n",
              " (-40, -795),\n",
              " (-39, -775),\n",
              " (-38, -755),\n",
              " (-37, -735),\n",
              " (-36, -715),\n",
              " (-35, -695),\n",
              " (-34, -675),\n",
              " (-33, -655),\n",
              " (-32, -635),\n",
              " (-31, -615),\n",
              " (-30, -595),\n",
              " (-29, -575),\n",
              " (-28, -555),\n",
              " (-27, -535),\n",
              " (-26, -515),\n",
              " (-25, -495),\n",
              " (-24, -475),\n",
              " (-23, -455),\n",
              " (-22, -435),\n",
              " (-21, -415),\n",
              " (-20, -395),\n",
              " (-19, -375),\n",
              " (-18, -355),\n",
              " (-17, -335),\n",
              " (-16, -315),\n",
              " (-15, -295),\n",
              " (-14, -275),\n",
              " (-13, -255),\n",
              " (-12, -235),\n",
              " (-11, -215),\n",
              " (-10, -195),\n",
              " (-9, -175),\n",
              " (-8, -155),\n",
              " (-7, -135),\n",
              " (-6, -115),\n",
              " (-5, -95),\n",
              " (-4, -75),\n",
              " (-3, -55),\n",
              " (-2, -35),\n",
              " (-1, -15),\n",
              " (0, 5),\n",
              " (1, 25),\n",
              " (2, 45),\n",
              " (3, 65),\n",
              " (4, 85),\n",
              " (5, 105),\n",
              " (6, 125),\n",
              " (7, 145),\n",
              " (8, 165),\n",
              " (9, 185),\n",
              " (10, 205),\n",
              " (11, 225),\n",
              " (12, 245),\n",
              " (13, 265),\n",
              " (14, 285),\n",
              " (15, 305),\n",
              " (16, 325),\n",
              " (17, 345),\n",
              " (18, 365),\n",
              " (19, 385),\n",
              " (20, 405),\n",
              " (21, 425),\n",
              " (22, 445),\n",
              " (23, 465),\n",
              " (24, 485),\n",
              " (25, 505),\n",
              " (26, 525),\n",
              " (27, 545),\n",
              " (28, 565),\n",
              " (29, 585),\n",
              " (30, 605),\n",
              " (31, 625),\n",
              " (32, 645),\n",
              " (33, 665),\n",
              " (34, 685),\n",
              " (35, 705),\n",
              " (36, 725),\n",
              " (37, 745),\n",
              " (38, 765),\n",
              " (39, 785),\n",
              " (40, 805),\n",
              " (41, 825),\n",
              " (42, 845),\n",
              " (43, 865),\n",
              " (44, 885),\n",
              " (45, 905),\n",
              " (46, 925),\n",
              " (47, 945),\n",
              " (48, 965),\n",
              " (49, 985)]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps for the following code\n",
        "\n",
        "* inputs are list of tuples of [(x1,y1)...(xn,yn)]\n",
        "* For one data point:\n",
        "$L_i(m,b) = (mx_i + b - y_i)^2$\n",
        "* For all data points, the objective is usually the mean squared error:\n",
        "$$L(m,b) = \\frac{1}{n}\\sum_{i=1}^n L_i(m,b)$$\n",
        "* We calculate $\\partial L / \\partial m and \\partial L / \\partial b for each point$\n",
        "More precisely, we compute:\n",
        "$\\frac{\\partial L_i}{\\partial m}, \\quad \\frac{\\partial L_i}{\\partial b}$\n",
        "* This’s what `linear_gradient(x, y, theta)` returns.\n",
        "* Each point has a 2D gradient vector. Exactly. each data point produces:\n",
        "$\\nabla L_i = [2(x_i)(mx_i+b-y_i),\\ 2(mx_i+b-y_i)]$\n",
        "  * A 2D vector because there are two parameters.\n",
        "* `grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])`\n",
        "  * (e) Why do we average all gradients? (this is the key conceptual step)\n",
        "  \n",
        "  This part is subtle but crucial.\n",
        "\n",
        "  What are we minimizing?\n",
        "\n",
        "  Not each $L_i$ individually — we want to minimize the average loss:\n",
        "\n",
        "  $L(m,b) = \\frac{1}{n}\\sum_{i=1}^n L_i(m,b)$\n",
        "\n",
        "  Gradient of a sum = sum of gradients\n",
        "\n",
        "  A fundamental calculus fact:\n",
        "  $\\nabla \\left(\\frac{1}{n}\\sum_{i=1}^n L_i\\right)\n",
        "  =\n",
        "  \\frac{1}{n}\\sum_{i=1}^n \\nabla L_i$\n",
        "\n",
        "  So:\n",
        "    * averaging gradients is not arbitrary\n",
        "    * it is exactly the gradient of the objective we care about\n",
        "* Update theta,\n",
        "  * `theta = gradient_step(theta, grad, -learning_rate)`\n",
        "  * This does: $\\theta \\leftarrow \\theta - \\eta \\nabla L$ ; why multiply by minus because we want to go downhill\n",
        "  * Where:\n",
        "\t  * $\\eta$ = learning rate\n",
        "\t  *\tmove against the gradient to go downhill\n",
        "\n",
        "  This slightly improves m and b.\n",
        "* This completes one epoch,\n",
        "  computes:\n",
        "$\\left[\n",
        "\\frac{\\partial L_i}{\\partial m},\n",
        "\\frac{\\partial L_i}{\\partial b}\n",
        "\\right]$"
      ],
      "metadata": {
        "id": "nkdZPjTuPlYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First \"Using Gradient Descent to Fit Models\" example\n",
        "\n",
        "from scratch.linear_algebra import vector_mean\n",
        "\n",
        "# Start with random values for slope and intercept.\n",
        "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epoch in range(5000):\n",
        "    # Compute the mean of the gradients\n",
        "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
        "    # Take a step in that direction\n",
        "    theta = gradient_step(theta, grad, -learning_rate)\n",
        "    #print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "print('Final theta is: ', theta)\n",
        "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9DMBgqg9PbR",
        "outputId": "7df95680-93a6-4e40-d2df-666745f27b83"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final theta is:  [19.999999856401352, 4.999760908167728]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First improvement using batches\n",
        "\n",
        "The improvement is how the gradient is computed and how often you update $\\theta$.\n",
        "\n",
        "In the previous (“batch gradient descent”) code, each epoch did:\n",
        "* compute gradients for all points\n",
        "* average them\n",
        "* one update of $\\theta$\n",
        "\n",
        "Here (“mini-batch gradient descent”), each epoch does:\n",
        "* split the data into small batches (here size 20)\n",
        "* for each batch:\n",
        "* compute gradients on just that batch\n",
        "* average them\n",
        "* update $\\theta$ immediately\n",
        "* so you get many updates per epoch"
      ],
      "metadata": {
        "id": "n7Xtpz1XTpua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous code (**batch GD**)\n",
        "\n",
        "Per epoch:\n",
        "* 1 gradient computed from N points\n",
        "* 1 update\n",
        "\n",
        "New code (**mini-batch GD**)[see below]\n",
        "\n",
        "Per epoch:\n",
        "* N / 20 gradients computed (one per batch)\n",
        "* N / 20 updates\n",
        "\n",
        "If you have 100 points, batch size 20 → 5 updates per epoch."
      ],
      "metadata": {
        "id": "M7w13Wn4UPDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minibatch gradient descent example\n",
        "\n",
        "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "\n",
        "for epoch in range(1000):\n",
        "    for batch in minibatches(inputs, batch_size=20):\n",
        "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
        "        theta = gradient_step(theta, grad, -learning_rate)\n",
        "    # print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "print('Final theta is: ', theta)\n",
        "\n",
        "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj_URj3M93UY",
        "outputId": "6e40544a-0d0d-4ace-da8d-ceb4a4d68ca6"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final theta is:  [20.000000050520605, 5.000000795801188]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What’s different vs mini-batch?\n",
        "\n",
        "* Batch size is 1 (one example at a time)\n",
        "\n",
        "In SGD, each update uses a single training example (x,y):\n",
        "\n",
        "```\n",
        "for x, y in inputs:\n",
        "    grad = linear_gradient(x, y, theta)\n",
        "    theta = gradient_step(theta, grad, -learning_rate)\n",
        "```\n",
        "So every iteration of that inner loop:\n",
        "* computes the gradient from one point\n",
        "* updates theta immediately\n",
        "\n",
        "This is equivalent to mini-batch with batch_size=1.\n",
        "\n",
        "* No vector_mean(...) needed\n",
        "\n",
        "Because you only have one gradient vector, there’s nothing to average.\n",
        "\n",
        "**Mini-batch:**\n",
        "\n",
        "```\n",
        "grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
        "```\n",
        "\n",
        "**SGD**\n",
        "\n",
        "```\n",
        "grad = linear_gradient(x, y, theta)\n",
        "```"
      ],
      "metadata": {
        "id": "5k3i8gfcXBom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to understand one “epoch” in SGD\n",
        "\n",
        "An epoch in that SGD code means:\n",
        "\n",
        "`“Make one pass through the full dataset, updating parameters after each example.”`\n",
        "\n",
        "So if inputs has 100 points:\n",
        "* one epoch = 100 updates\n",
        "* 100 epochs = 10,000 updates\n",
        "\n",
        "**This is why SGD often uses fewer epochs than batch GD: it already does many updates.**\n",
        "\n",
        "\n",
        "Why SGD is considered an “improvement” (in many settings)\n",
        "* Each update is cheap (uses one point)\n",
        "* You can start improving immediately without scanning all data\n",
        "* Scales well when you have huge datasets\n",
        "* The “noise” in the updates can help in complex, non-convex problems\n",
        "\n",
        "Tradeoff:\n",
        "* updates are noisier (loss may bounce around), but you often reach a good solution faster in practice."
      ],
      "metadata": {
        "id": "6J21MWgSXtdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic gradient descent example\n",
        "\n",
        "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "\n",
        "for epoch in range(100):\n",
        "    for x, y in inputs:\n",
        "        grad = linear_gradient(x, y, theta)\n",
        "        theta = gradient_step(theta, grad, -learning_rate)\n",
        "    # print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "print('Final theta is: ', theta)\n",
        "\n",
        "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWLn7rN8E6q5",
        "outputId": "409db1f9-4471-4556-da4f-6592b4b88a30"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final theta is:  [20.001500348671083, 4.925323335098984]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Differential Coefficient Code\n",
        "\n",
        "The following code numerically approximates the partial derivative of a multivariable function with respect to one coordinate.\n",
        "\n",
        "$$ \\frac{\\partial f}{\\partial v_i}(v) \\;\\approx\\; \\frac{f(v + h\\,e_i) - f(v)}{h} $$\n",
        "\n",
        "\n",
        "Note:\n",
        "\n",
        "* f(v) -> function value at original point\n",
        "* f(w) -> function value at a small step in direction i\n",
        "* subtract -> change in function value\n",
        " divide by h -> rate of change\n",
        "* Key line of the code\n",
        "```\n",
        "v = [2.0, 3.0, 5.0]\n",
        "i = 1\n",
        "h = 0.001\n",
        "```\n",
        "Then the code will change\n",
        "\n",
        "```\n",
        "w = [2.0, 3.001, 5.0]\n",
        "```\n",
        "\n",
        "# Why this function works without calculus formuls\n",
        "\n",
        "This function:\n",
        "* Does not know the derivative of f\n",
        "* Does not assume f has a symbolic form\n",
        "* only needs to evaluate f\n",
        "\n",
        "# Nutshell\n",
        "* This fun approximates gradient numerically\n",
        "* Even when derivatives are hard or unknown (even without closed-form derivatives)\n",
        "\n",
        "`\n",
        "This function approximates a partial derivative by nudging one coordinate slightly and measuring how much the function value changes.\n",
        "`\n",
        "\n",
        "```\n",
        "def partial_difference_quotient(f: Callable[[Vector], float],\n",
        "                                v: Vector,\n",
        "                                i: int,\n",
        "                                h: float) -> float:\n",
        "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
        "    w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\n",
        "          for j, v_j in enumerate(v)]\n",
        "    return (f(w) - f(v)) / h\n",
        "```"
      ],
      "metadata": {
        "id": "nmtNbzp7cerI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfaCd0yJcjq3"
      },
      "execution_count": 84,
      "outputs": []
    }
  ]
}